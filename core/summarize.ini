<?php
require __DIR__.'/../vendor/autoload.php';

use Rindow\NeuralNetworks\Support\GenericUtils;
use Interop\Polite\Math\Matrix\NDArray;
use Rindow\NeuralNetworks\Layer\AbstractRNNLayer;
use Rindow\NeuralNetworks\Model\AbstractModel;
use Rindow\Math\Matrix\MatrixOperator;
use Rindow\Math\Plot\Plot;
use Rindow\NeuralNetworks\Backend\RindowBlas\Backend;
use Rindow\NeuralNetworks\Builder\NeuralNetworks;
use Rindow\NeuralNetworks\Data\Sequence\Tokenizer;
use Rindow\NeuralNetworks\Data\Sequence\Preprocessor;

$mo = new MatrixOperator();
$nn = new NeuralNetworks($mo);
$plt = new Plot(null,$mo);
$REVERSE = True;
$WORD_VECTOR = 16;
$UNITS = 128;

$dataset = fopen("/C:/neuralnetwork/vendor/rindow/rindow-neuralnetworks/src/Dataset/data.xlsx","r"); 
    $dataset = $nn->data->TextClassifiedDataset(
        $datasetdir.'/train',
        ['pattern'=>'@[0-9_]*\\.txt@','maxlen'=>256,'num_words'=>10000,
        'shuffle'=>true,'verbose'=>1]);
    [$train_text,$train_summary] = $dataset->loadData();
    $classnames = $dataset->classnames();
    $tokenizer = $dataset->getTokenizer();
    $dataset = $nn->data->TextClassifiedDataset(
        $datasetdir.'/test',
        ['pattern'=>'@[0-9_]*\\.txt@','tokenizer'=>$tokenizer,'maxlen'=>256,'num_words'=>10000,
        'shuffle'=>true,'verbose'=>1]);
    [$test_inputs,$test_labels] = $dataset->loadData();
    $train_summary = $mo->la()->astype($train_labels,NDArray::float32);
    $test_summary = $mo->la()->astype($test_labels,NDArray::float32);
    $savedata = [
        $tokenizer->save(),
        $train_text,$train_summary,
        $test_text,$test_summary,
    ];
    file_put_contents($savefilename,serialize($savedata));
} else {
    $savedata =  unserialize(file_get_contents($savefilename));
    [
        $tokenizer_data,
        $train_text,$train_summary,
        $test_text,$test_summary,
    ] = $savedata;
    $tokenizer = $nn->data->TextClassifiedDataset($datasetdir.'/train')
                    ->getTokenizer();
    $tokenizer->load($tokenizer_data);
}
$train_summary = $mo->la()->astype($train_labels,NDArray::float32);
$test_summary = $mo->la()->astype($test_labels,NDArray::float32);
echo implode(',',$train_text->shape())."\n";
echo implode(',',$train_summary->shape())."\n";
echo implode(',',$test_text->shape())."\n";
echo implode(',',$test_summary->shape())."\n";
$total_size = count($train_inputs);
$train_size = (int)floor($total_size*0.9);
$val_text = $train_text[[$train_size,$total_size-1]];
$val_summary = $train_summary[[$train_size,$total_size-1]];
$train_text = $train_text[[0,$train_size-1]];
$train_summary = $train_summary[[0,$train_size-1]];

$max_summary_len=15
$max_text_len=50


$modelFilePath = __DIR__."/data.model";

if(file_exists($modelFilePath)) {
    echo "loading model ...\n";
    $model = $nn->models()->loadModel($modelFilePath);
    $model->summary();
} else {
    $inputlen = $train_text->shape()[1];
    echo "creating model ...\n";
    $model = $nn->models()->Sequential([
        $nn->layers()->Embedding(count($input_dic), $WORD_VECTOR,
            ['input_length'=>$input_length]
        ),
        # Encoder
        $nn->layers()->GRU($UNITS,['go_backwards'=>$REVERSE]),
        # Expand to answer length and peeking hidden states
        $nn->layers()->RepeatVector($output_length),
        # Decoder
        $nn->layers()->GRU($UNITS, [
            'return_sequences'=>true,
            'go_backwards'=>$REVERSE,
        ]),
        # Output
        $nn->layers()->Dense(
            count($target_dic),
            ['activation'=>'softmax']
        ),
    ]);
    $model->compile([
        'loss'=>$nn->losses->BinaryCrossEntropy(),
        'optimizer'=>'adam',
    ]);
    $model->summary();
    echo "training model ...\n";
    $history = $model->fit($train_text,$train_summary,[
        'epochs'=>10,'batch_size'=>64,
        'validation_data'=>[$val_text,$val_summary],
    ]);
    $model->save($modelFilePath);
    
$model->save('summarizer.ini');
]);
